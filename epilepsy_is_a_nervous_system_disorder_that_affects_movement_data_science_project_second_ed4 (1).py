# -*- coding: utf-8 -*-
"""Epilepsy is a nervous system disorder that affects movement. data science project second ed4.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1AIjTCss5KC_ll6hqA9dOKnJd-hZuRBVg
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
warnings.filterwarnings('ignore')

data = pd.read_csv('https://raw.githubusercontent.com/edyoda/data-science-complete-tutorial/master/Data/epilepsy.data')

data.head()

data.info()

data.dtypes

data.describe()

data.isna().sum()

data.dropna(inplace=True)

data.isna().sum()

data.corr()

plt.figure(figsize=(12,10), dpi= 80)
sns.heatmap(data.corr())

plt.title('Correlation of features', fontsize=22)
plt.xticks(fontsize=12)
plt.yticks(fontsize=12)
plt.show()

data.skew()

plt.figure(figsize=(14,12), dpi= 80)
data.hist(sharex=False, sharey=False, xlabelsize=1, ylabelsize=1)
plt.show()

for i in data.columns:
    print(i,' -> ',data[i].nunique())

data.drop('name',axis=1,inplace=True)

data.head()

e = data.drop('status',axis=1)
f = data['status']

from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test = train_test_split(e,f,test_size=0.2,random_state=5,shuffle=True)

print(x_train.shape,y_train.shape,x_test.shape,y_test.shape)

from sklearn.linear_model import LogisticRegression
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.model_selection import KFold
from sklearn.model_selection import cross_val_score

mdls = []
mdls.append(('lr',LogisticRegression()))
mdls.append(('lda',LinearDiscriminantAnalysis()))
mdls.append(('knn',KNeighborsClassifier()))
mdls.append(('dt',DecisionTreeClassifier()))
mdls.append(('nb',GaussianNB()))
mdls.append(('svm',SVC()))
for name,model in mdls:
    kfold = KFold(n_splits=10, shuffle=True, random_state=None)
    rslts = cross_val_score(model,x_train,y_train,scoring='accuracy',cv=kfold)
    print(name,' -> ', np.mean(rslts))

from sklearn.preprocessing import StandardScaler
es = StandardScaler()
scal_xtrain = es.fit_transform(x_train)
scal_xtrain

for a, b in mdls:
    kfold = KFold(n_splits=10, shuffle=True)
    rslts_1 = cross_val_score(b, scal_xtrain, y_train, cv=kfold, scoring='accuracy')
    print(a, '->', np.mean(rslts_1))

from sklearn.model_selection import GridSearchCV
neighbors = [1,3,5,7,9,11,13,15,17,19,21]
param_grid = dict(n_neighbors=neighbors)
model = KNeighborsClassifier()
kfold = KFold(n_splits=10, shuffle=True, random_state=4)

grid = GridSearchCV(estimator=model, param_grid=param_grid, scoring='accuracy')

grid.fit(scal_xtrain,y_train)

grid.best_score_

grid.best_params_

for index,i in enumerate(grid.cv_results_['mean_test_score']):
    print(grid.cv_results_['params'][index], "->",i)

param_grid = [{'max_features':[8,9,10,11],'min_samples_split':[2,3,4,5]}]
grid = GridSearchCV(DecisionTreeClassifier(),param_grid)

grid.fit(scal_xtrain,y_train)

grid.best_score_

grid.best_params_

from sklearn.ensemble import BaggingClassifier
kfold = KFold(n_splits=10, shuffle=True, random_state=9)
bagging = BaggingClassifier(base_estimator=DecisionTreeClassifier(),n_estimators=100,random_state=8)
results=cross_val_score(bagging,scal_xtrain,y_train,cv=kfold,scoring='accuracy')
print(np.mean(results),np.std(results))

from sklearn.ensemble import RandomForestClassifier
rfc = RandomForestClassifier()
results = cross_val_score(rfc,scal_xtrain,y_train,scoring='accuracy')

print(np.mean(results),np.std(results))

from sklearn.ensemble import AdaBoostClassifier
boosting = AdaBoostClassifier(n_estimators=100,random_state=10)
result=cross_val_score(boosting,scal_xtrain,y_train,cv=kfold,scoring='accuracy')

print(np.mean(results),np.std(results))

from sklearn.ensemble import VotingClassifier
models = []
models.append(('knn',KNeighborsClassifier(n_neighbors=1)))
models.append(('svc',SVC()))
models.append(('dt',DecisionTreeClassifier(max_features=9,min_samples_split=4)))
ensemble = VotingClassifier(models)
results = cross_val_score(ensemble,scal_xtrain,y_train,cv=kfold,scoring='accuracy')

print(np.mean(results),np.std(results))

models

from sklearn.metrics import classification_report
from sklearn.metrics import accuracy_score
ensemble.fit(scal_xtrain,y_train)
ypred = ensemble.predict(x_test)

accuracy_score(y_test,ypred)

boost=AdaBoostClassifier(n_estimators=100,random_state=16)
boost.fit(scal_xtrain,y_train)
y_pred_boost = boost.predict(x_test)

accuracy_score(y_test,y_pred_boost)

knn = KNeighborsClassifier(n_neighbors=1)
knn.fit(scal_xtrain,y_train)
y_pred_knn = knn.predict(x_test)

accuracy_score(y_test,y_pred_knn)