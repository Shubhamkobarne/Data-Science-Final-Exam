# -*- coding: utf-8 -*-
"""Predicting Shopping Mall Sales Data Science Project

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Pq8yTU-woP2Bwbbcy8HPElrJoLgeFafC
"""

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.feature_selection import SelectKBest
from sklearn.feature_selection import chi2
from sklearn.model_selection import train_test_split
from sklearn.feature_selection import RFECV
from sklearn.tree import DecisionTreeRegressor
from sklearn.pipeline import Pipeline
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import KFold
import warnings
warnings.filterwarnings('ignore')

data = pd.read_csv('https://raw.githubusercontent.com/edyoda/data-science-complete-tutorial/master/Data/Shopping_Revenue.csv')

data.head()

data.shape

for i in data.columns:
    print(i,data[i].nunique())

data.drop(['Id','Open Date','City'],axis=1,inplace=True)

data.head()

data['Type'].unique()

data['City Group'] = data['City Group'].map({'Big Cities':0,'Other':1})
data['Type'] = data['Type'].map({'IL':0, 'FC':1, 'DT':2})

data.head()

data.info()

data.isna().sum()

data.dropna(inplace=True)

data.describe()

for i in data.columns:
    print(np.var(data[i]))

data.corr()

from sklearn.preprocessing import StandardScaler
values = data.values
sd = StandardScaler()
y_scle = sd.fit_transform(values[:,39].reshape(-1,1))

data['revenue'] = y_scle

i = data.drop('revenue',axis=1)
j = data['revenue']

x_train,x_test,y_train,y_test = train_test_split(i,j,test_size=0.2,random_state=1)

print(x_train.shape,x_test.shape,y_train.shape,y_test.shape)

from sklearn.model_selection import RepeatedKFold
from sklearn.feature_selection import RFE
from sklearn.metrics import mean_squared_error
Rfe = RFE(estimator=DecisionTreeRegressor(), n_features_to_select=10)
Decisiontree_model = DecisionTreeRegressor()
pipeline = Pipeline(steps=[('s',Rfe),('m',Decisiontree_model)])
cc = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)
Na_scores = cross_val_score(pipeline, x_train, y_train, cv=cc)
Na_scores

print(np.mean(Na_scores))

Na_features = [10,13,15,18,22,25,28,32]
for i in Na_features:
    Rfe = RFE(estimator=DecisionTreeRegressor(), n_features_to_select=i)
    Decisiontree_model = DecisionTreeRegressor()
    pipeline = Pipeline(steps=[('s',Rfe),('m',Decisiontree_model)])
    cc = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)
    Na_scores = cross_val_score(pipeline, x_train, y_train, cv=cc,scoring='r2')
    print(i,np.mean(Na_scores),np.std(Na_scores))

from sklearn.ensemble import RandomForestRegressor
from sklearn.feature_selection import RFE
Rfr = RandomForestRegressor()
Rfe = RFE(estimator=Rfr, n_features_to_select=22)
fs_xtrain = Rfe.fit_transform(x_train, y_train)
print(Rfe.support_)
print(Rfe.ranking_)

from sklearn.linear_model import LinearRegression
from sklearn.neighbors import KNeighborsRegressor
from sklearn.tree import DecisionTreeRegressor
from sklearn.svm import SVR
Mdls = []
Mdls.append(('LR',LinearRegression()))
Mdls.append(('knn',KNeighborsRegressor()))
Mdls.append(('dt',DecisionTreeRegressor()))
Mdls.append(('svm',SVR()))

for e,f in Mdls:
    kfold = KFold(n_splits=3, shuffle=False)
    results_e = cross_val_score(f, x_train, y_train, scoring='neg_mean_squared_error', cv=kfold)
    print(e,np.mean(results_e))

for name,mdl in Mdls:
    kfold = KFold(n_splits=3, shuffle=True, random_state=1)
    results_f = cross_val_score(mdl,fs_xtrain,y_train,cv=kfold,scoring = 'neg_mean_squared_error')
    print(name,np.mean(results_f))

prm = [1,3,5,7,9,11,13,15,17]
for i in prm:
    pipeline_1 = []
    mdl1 = KNeighborsRegressor(n_neighbors=i)
    kfold = KFold(n_splits=10, shuffle=True, random_state=1)
    rslt= cross_val_score(mdl1,x_train,y_train,cv=kfold,scoring = 'neg_mean_squared_error')
    print(i,np.mean(rslt))

from sklearn.ensemble import RandomForestRegressor
from sklearn.ensemble import AdaBoostRegressor
from sklearn.ensemble import GradientBoostingRegressor
ensemble = [('rfc',RandomForestRegressor()),('ab',AdaBoostRegressor()),('gb',GradientBoostingRegressor())]
for x,y in ensemble:
    kfold = KFold(n_splits=3, shuffle=True, random_state=8)
    ensemble_rslt = cross_val_score(y,fs_xtrain,y_train,cv=kfold,scoring = 'neg_mean_squared_error')
    print(x,np.mean(ensemble_rslt))

from sklearn.model_selection import GridSearchCV
sd=7
scoring = 'neg_mean_squared_error'
prm_grd = dict(n_estimators=np.array([50,75,100,125,150,200]))
mdl = GradientBoostingRegressor(random_state=sd)
Kfold = KFold(n_splits=3, shuffle=True, random_state=sd)
grd = GridSearchCV(estimator=mdl, param_grid=prm_grd, scoring=scoring, cv=Kfold)
grd_rslt = grd.fit(x_train, y_train)

grd.best_score_

grd.cv_results_['params']

grd.cv_results_['mean_test_score']

from sklearn.ensemble import VotingRegressor
st = VotingRegressor([('svm',SVR()),('knn',KNeighborsRegressor(n_neighbors=13))])
k_fold = KFold(n_splits=3, shuffle=True, random_state=98)
rslt_vot = cross_val_score(st,fs_xtrain,y_train,cv=k_fold,scoring='neg_mean_squared_error')

print(np.mean(rslt_vot))

from sklearn.metrics import mean_squared_error

VR = VotingRegressor([('svm',SVR()),('knn',KNeighborsRegressor(n_neighbors=13))])
VR.fit(x_train,y_train)
ypred = VR.predict(x_test)

mean_squared_error(y_test,ypred)





